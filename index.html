<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script -->

  <title>Ankur Kumar</title>
  
  <meta name="author" content="Ankur Kumar">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ankur Kumar</name>
              </p>
              <p style="text-align:justify">I am a research engineer at <a href="https://research.samsung.com/sri-b">Samsung R&D Institute, Bangalore</a>, where I work on End-to-End Speech Recognition for Bixby in collaboration with the Speech Processing Lab at Samsung Research, Korea.
              </p>
	      <p style="text-align:justify">Prior to this, I graduated from <a href="http://iitk.ac.in">Indian Institute of Technology Kanpur</a> with a major in Computer Science and Engineering.
              </p>
	      <p style="text-align:justify">Research Interests: Deep Learning, Generative Modeling, Unsupervised Learning
              </p>
              <p style="text-align:center">
                <a href="mailto:ankur26196@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/Ankur-CV.pdf">CV</a> &nbsp/&nbsp
                <!-- a href="data/Ankur-bio.txt">Biography</a -->
                <!-- a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a --> 
                <a href="http://www.linkedin.com/in/ankur-kumar-049997b8"> LinkedIn </a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ankur.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ankur.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
	
	<!-- UPDATES SECTION -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Updates</heading>
            </td>
          </tr>
            <td><ul style="list-style-type:circle;">
		    <li>[Dec 2019] An on-device version of our streaming E2E ASR model was released for Samsung's flagship smartphones</li>
		</ul>
            </td>
        </tbody></table>

	<!-- RESEARCH SECTION -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- ASRU 2019 PAPER  -->
          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nightsight_image'><img src='images/ASRU19.jpg' style="height: 80%; width: 100%"></div>
                <img src='images/ASRU19.jpg' style="height: 80%; width: 100%">
              </div>
              <script type="text/javascript">
                function nightsight_start() {
                  document.getElementById('nightsight_before').style.opacity = "1";
                }

                function nightsight_stop() {
                  document.getElementById('nightsight_after').style.opacity = "0";
                }
                nightsight_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
		<!-- a href=https://cmsworkshops.com/ASRU2019/Papers/ViewPaper.asp?PaperNum=1066 -->
		<a href="data/ASRU2019.pdf">
                <papertitle>Improved Multi-stage Training of Online Attention-based Encoder-Decoder Models</papertitle>
		</a>
              <br>
              <!-- em>Abhinav Garg, Dhananjaya Gowda, Ankur Kumar, Kwangyoun Kim, Mehul Kumar, Chanwoo Kim</em --><em>IEEE Automatic Speech Recognition and Understanding Workshop, 2019</em>
              <p></p>
              <p style="text-align:justify">In this paper, we propose a refined multi-stage multi-task training strategy to improve the performance of online attention-based encoder-decoder models. Our experiments involve two-levels of linguistic granularity namely, character and BPE. We explore different pre-training strategies for the encoders including transfer learning from a bidirectional encoder. Our encoder-decoder models with online attention show 35% and 10% improvement over their baselines for smaller and bigger models, respectively. Our models achieve a word error rate of 5.04% and 4.48% on the Librispeech test-clean data for the smaller and bigger models respectively after fusion with LSTM-based external language model.</p>
            </td>
          </tr>

          <!-- INTERN -->
          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nightsight_image'><img src='images/Adobe.jpg' style="height: 50%; width: 50%"></div>
                <img src='images/Adobe.jpg' style="height: 50%; width: 50%">
              </div>
              <script type="text/javascript">
                function nightsight_start() {
                  document.getElementById('nightsight_before').style.opacity = "1";
                }

                function nightsight_stop() {
                  document.getElementById('nightsight_after').style.opacity = "0";
                }
                nightsight_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Objective-driven Video Ad Generation for Brands using Deep Learning Techniques</papertitle>
              <br>
              <em>Research Intern at Adobe Systems</em>, 2017
              <p></p>
              <p style="text-align:justify"> The work served as a proof of concept to leverage the vast amount of unused data using weak supervision. Specifically, I worked to utilize social media videos to create promotional videos for brands. I built an RNN-based autoregressive model that would output a sequence of features based on a training corpus of brand videos. Using these features, I extracted relevant video segments from a pool of social media videos via clustering techniques and used those segments to stitch a new video.</p>
            </td>
          </tr>
          
	<!-- PROJECTS SECTION -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Projects</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	  <!-- NATURAL LANGUAGE PROCESSING (CS671) -->
          <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='font_image'><img src='images/CS671.jpg' style="height: 100%; width: 120%"></div>
                <img src='images/CS671.jpg' style="height: 100%; width: 120%">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/CS671-FactoidQuestionGeneration-Report.pdf">
                <papertitle>Factoid Question Generation from Paragraph</papertitle>
              </a>
              <br>
              <em>Natural Language Processing Course Project</em>, 2018 &nbsp <!-- font color="red"><strong>(Oral Presentation)</strong></font -->
              <br>
              <p></p>
              <p style="text-align:justify">We built a pipeline for fact-based question generation with paragraph as input. To this end, we constructed a sentence classifier, which would classify a sentence containing facts as positive with 71% accuracy over SQuAD dataset. These positive sentences and corresponding questions were used to train an attention-based encoder-decoder model. We also experimented with different attention mechanisms discussed in the paper "Effective Approaches to Attention-based Neural Machine Translation".</p>
            </td>
          </tr>

	  <!-- TOPICS IN PROBABILISTIC MODELING AND INFERENCE (CS698X) -->
          <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='font_image'><img src='images/AVB.jpg' style="height: 80%; width: 120%"></div>
                <img src='images/AVB.jpg' style="height: 80%; width: 120%">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/CS698X-AVB.pdf">
                <papertitle>Adversarial Variational Bayes in Edward</papertitle>
              </a>
              <br>
              <em>Topics in Probabilistic Modeling and Inference Course Project</em>, 2018 &nbsp <!-- font color="red"><strong>(Oral Presentation)</strong></font -->
              <br>
              <p></p>
              <p style="text-align:justify">The objective was to explore Edward, a probabilistic modeling framework in Python, for its high level abstraction to random variables and inferences as well as its plug and play architecture. We implemented Adversarial Variational Bayes (AVB) in Edward and obtained results comparable with that in the paper by Mescheder et. al. on binarized MNIST dataset.</p>
            </td>
          </tr>

	  <!-- VISUAL RECOGNITION COURSE (CS698O) -->
          <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='font_image'><img src='images/CS698O.jpg' style="height: 80%; width: 120%"></div>
                <img src='images/CS698O.jpg' style="height: 80%; width: 120%">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/CS698O-DomainAdaptation-Report.pdf">
                <papertitle>Unsupervised Pixel–Level Domain Adaptation with
Generative Adversarial Networks</papertitle>
              </a>
              <br>
              <em>Visual Recognition Course Project</em>, 2017 &nbsp <!-- font color="red"><strong>(Oral Presentation)</strong></font -->
              <br>
              <p></p>
              <p style="text-align:justify">We experimented with different GAN architectures for domain adaptation using classification task. Our result confirms that the proposed model, PixelDA-GAN, outperforms the baseline, which is a model trained on source only dataset. However, we also found that replacing PixelDA GAN with LS-GAN resulted in even better performance.</p>
	    <p align="right"><font size="0">
	    <a href="http://www.cs.berkeley.edu/~barron">Thanks to Jon Barron!</a>
	    </font></p>
            </td>
          </tr>
          
        </tbody></table>
</body>

</html>
